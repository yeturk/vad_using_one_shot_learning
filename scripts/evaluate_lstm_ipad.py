import numpy as np
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
import matplotlib.pyplot as plt

print("\n ************    0. evaluate_lstm_ipad.py is executed by yet :)    ************")

# ======================================================
# 1ï¸âƒ£ Load anomaly scores & ground truth
# ======================================================
scores = np.load("/home/yunus/projects/vad_using_one_shot_learning/data/results/anomaly_scores_R01.npy")
labels = np.load("/home/yunus/projects/vad_using_one_shot_learning/dataset/IPAD_dataset/R01/test_label/001.npy")

# Ä°kisini hizalamak iÃ§in sequence offset dÃ¼zeltmesi
# (her sequence 10 frame olduÄŸundan son 10 frame labeli dÃ¼ÅŸer)
min_len = min(len(scores), len(labels))
scores = scores[:min_len]
labels = labels[:min_len]

print(f"âœ… Scores shape: {scores.shape}, Labels shape: {labels.shape}")

# ======================================================
# 2ï¸âƒ£ Normalize Scores
# ======================================================
scores_norm = (scores - np.min(scores)) / (np.max(scores) - np.min(scores))

# ======================================================
# 3ï¸âƒ£ Determine threshold
# ======================================================

# option-1
# threshold = np.percentile(scores_norm, 90)
# print(f"âš™ï¸  Threshold (90 percentile): {threshold:.4f}")

# option-2
# ======================================
#  Compute Threshold using Z-score rule
# ======================================
# mean = np.mean(scores_norm)
# std = np.std(scores_norm)
# threshold = mean + 1.5 * std  # or try 1.2 â€“ 1.8 range for tuning

# print(f"Mean: {mean:.3f}, Std: {std:.3f}, Threshold (Z-Score): {threshold:.3f}")

# option-3
from sklearn.metrics import precision_recall_curve, f1_score

precisions, recalls, thresholds = precision_recall_curve(labels, scores_norm)
f1s = 2 * (precisions * recalls) / (precisions + recalls)
best_idx = np.argmax(f1s)
best_thresh = thresholds[best_idx]
print(f"ðŸ” Best threshold for max F1: {best_thresh:.3f}")

threshold = best_thresh
preds = (scores_norm > threshold).astype(int)

# ======================================================
# 4ï¸âƒ£ Metrics
# ======================================================
acc = accuracy_score(labels, preds)
prec = precision_score(labels, preds, zero_division=0)
rec = recall_score(labels, preds, zero_division=0)
f1 = f1_score(labels, preds, zero_division=0)
auc = roc_auc_score(labels, scores_norm)

print(f"""
ðŸ“Š Evaluation Metrics:
-----------------------
âœ… Accuracy : {acc:.3f}
âœ… Precision: {prec:.3f}
âœ… Recall   : {rec:.3f}
âœ… F1 Score : {f1:.3f}
âœ… ROC-AUC  : {auc:.3f}
""")

# ======================================================
# 5ï¸âƒ£ Visualization
# ======================================================
plt.figure(figsize=(12,5))
plt.plot(scores_norm, label="Anomaly Score (normalized)")
plt.axhline(y=threshold, color="r", linestyle="--", label=f"Threshold={threshold:.3f}")
plt.fill_between(np.arange(len(labels)), 0, 1, where=labels>0, color='red', alpha=0.2, label="Ground Truth Anomaly")
plt.xlabel("Sequence Index (time)")
plt.ylabel("Normalized Error")
plt.title("Ground Truth vs Predicted Anomaly Scores")
plt.legend()
plt.grid(True)
plt.show()